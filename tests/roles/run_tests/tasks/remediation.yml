---
  - name: Get m3m object
    k8s_info:
      api_version: infrastructure.cluster.x-k8s.io/{{ CAPM3_VERSION }}
      kind: Metal3Machine
      namespace: "{{ NAMESPACE }}"
    register: m3m

  - name: Get control-plane and worker objects
    set_fact:
      controlplanes: "{{ m3m.resources | json_query(q_controlplanes) }}"
      workers: "{{ m3m.resources | json_query(q_workers) }}"
    vars:
      q_controlplanes: "[? contains(metadata.name,'controlplane')].[metadata.annotations.\"metal3.io/BareMetalHost\", metadata.ownerReferences[? kind=='Machine']|[0].name]"
      q_workers: "[? contains(metadata.name,'workers')].[metadata.annotations.\"metal3.io/BareMetalHost\", metadata.ownerReferences[? kind=='Machine']|[0].name][0]"

  - set_fact:
      WORKER_BMH: "{{ workers[0] | replace('metal3/','')}}"
      WORKER_NODE: "{{ workers[1] }}"
      CONTROLPLANE_BMH_0: "{{ controlplanes[0][0] | replace('metal3/','')}}"
      CONTROLPLANE_NODE_0: "{{ controlplanes[0][1] }}"
      CONTROLPLANE_BMH_1: "{{ controlplanes[1][0] | replace('metal3/','')}}"
      CONTROLPLANE_NODE_1: "{{ controlplanes[1][1] }}"
      CONTROLPLANE_BMH_2: "{{ controlplanes[2][0] | replace('metal3/','')}}"
      CONTROLPLANE_NODE_2: "{{ controlplanes[2][1] }}"
      CONTROLPLANE_VM_0: "{{ controlplanes[0][0] | replace('-','_') | replace('metal3/','') }}"
      CONTROLPLANE_VM_1: "{{ controlplanes[1][0] | replace('-','_') | replace('metal3/','') }}"
      CONTROLPLANE_VM_2: "{{ controlplanes[2][0] | replace('-','_') | replace('metal3/','') }}"
      WORKER_VM: "{{ workers[0] | replace('-','_') | replace('metal3/','') }}"
      NUMBER_OF_BMH: "{{ NUM_OF_CONTROLPLANE_REPLICAS|int +  NUM_OF_WORKER_REPLICAS|int }}"

  - name: Fetch the target cluster kubeconfig
    k8s_info:
      kind: secrets
      name: "{{ CLUSTER_NAME }}-kubeconfig"
      namespace: "{{ NAMESPACE }}"
    register: metal3_kubeconfig

  - name: Decode and save cluster kubeconfig
    copy:
      content: "{{ metal3_kubeconfig.resources[0].data.value | b64decode }}"
      dest: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Reboot a single worker node
    include_tasks: reboot.yml
    vars:
      BMH_NODE: "{{ WORKER_BMH }}"
      LIBVIRT_VM: "{{ WORKER_VM }}"
      K8S_NODE: "{{ WORKER_NODE }}"

  - name: Power cycle a single worker node
    include_tasks: power_cycle.yml
    vars:
      BMH_NODE: "{{ WORKER_BMH }}"
      LIBVIRT_VM: "{{ WORKER_VM }}"
      K8S_NODE: "{{ WORKER_NODE }}"

  - name: Power cycle a single control-plane node
    include_tasks: power_cycle.yml
    vars:
      BMH_NODE: "{{ CONTROLPLANE_BMH_0 }}"
      LIBVIRT_VM: "{{ CONTROLPLANE_VM_0 }}"
      K8S_NODE: "{{ CONTROLPLANE_NODE_0 }}"

  # Power cycle two control-plane nodes
  - name: Power off "{{ CONTROLPLANE_BMH_1 }}" and "{{ CONTROLPLANE_BMH_2 }}"
    k8s:
      state: present
      namespace: "{{ NAMESPACE }}"
      definition:
        apiVersion: metal3.io/v1alpha1
        kind: BareMetalHost
        metadata:
          name: "{{ item }}"
          annotations:
            reboot.metal3.io/poweroff: ""
    with_items:
      - "{{ CONTROLPLANE_BMH_1 }}"
      - "{{ CONTROLPLANE_BMH_2 }}"

  - pause:
      minutes: 1

  - name: List only powered off VMs
    virt:
      command: list_vms
      state: shutdown
    register: shutdown_vms
    retries: 50
    delay: 10
    until:
      - CONTROLPLANE_VM_1 in shutdown_vms.list_vms
      - CONTROLPLANE_VM_2 in shutdown_vms.list_vms
    become: yes
    become_user: root

  - name: Power on controlplanes
    k8s:
      state: present
      namespace: "{{ NAMESPACE }}"
      definition:
        apiVersion: metal3.io/v1alpha1
        kind: BareMetalHost
        metadata:
          name: "{{ item }}"
          annotations:
            reboot.metal3.io/poweroff: null
    with_items:
      - "{{ CONTROLPLANE_BMH_1 }}"
      - "{{ CONTROLPLANE_BMH_2 }}"

  - name: Wait until powered on control-plane nodes become Ready
    shell: "kubectl get nodes --kubeconfig /tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml | grep -w Ready | awk '{print $1}' | sort"
    register: ready_controlplane
    retries: 150
    delay: 3
    until:
      - CONTROLPLANE_NODE_1 in ready_controlplane.stdout_lines
      - CONTROLPLANE_NODE_2 in ready_controlplane.stdout_lines

  - name: List only running VMs
    virt:
      command: list_vms
      state: running
    register: running_vms
    retries: 50
    delay: 10
    until:
      - CONTROLPLANE_VM_1 in running_vms.list_vms
      - CONTROLPLANE_VM_2 in running_vms.list_vms
    become: yes
    become_user: root

  # Start Unhealthy node testing
  - name: Scale KCP down to one replica
    k8s:
      api_version: controlplane.cluster.x-k8s.io/{{ CAPI_VERSION }}
      kind: KubeadmControlPlane
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      definition:
        spec:
          replicas: 1

    # we use shell instead of k8s_info module here because the CP may not respond to requests
    # while it is scaling down - k8s_info has no request timeout, which can hang the build
  - name: Wait until KCP is scaled down and two hosts are available
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | egrep -c '\b(available)|(ready)\b'
    retries: 200
    delay: 20
    register: available_bmhs
    until: available_bmhs.stdout == "2"

  - name: Mark "{{ WORKER_BMH }}" as unhealthy
    k8s:
      state: present
      namespace: "{{ NAMESPACE }}"
      definition:
        apiVersion: metal3.io/v1alpha1
        kind: BareMetalHost
        metadata:
          name: "{{ WORKER_BMH }}"
          annotations:
            capi.metal3.io/unhealthy: ""

  - name: Delete worker Machine object "{{ WORKER_NODE }}"
    k8s:
      state: absent
      api_version: cluster.x-k8s.io/{{ CAPI_VERSION }}
      kind: machines
      name: "{{ WORKER_NODE }}"
      namespace: "{{ NAMESPACE }}"

  - name: Wait until worker BMH is in available state
    k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
    register: bmhs
    retries: 200
    delay: 20
    until:
      - bmhs is succeeded
      - WORKER_BMH in (bmhs.resources | filter_provisioning("available,ready") | map(attribute='metadata.name'))

  - name: Wait until two BMH are provisioned
    k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
    register: bmhs
    retries: 200
    delay: 20
    until:
      - bmhs is succeeded
      - bmhs.resources | filter_provisioning("provisioned") | length == 2

  - name: Scale up the machinedeployment to 3 replicas
    k8s:
      api_version: cluster.x-k8s.io/{{ CAPI_VERSION }}
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      definition:
        spec:
          replicas: 3

  - pause:
      minutes: 1

  - name: Wait and verify that only one node starts provisioning
    k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
    register: bmhs
    retries: 10
    delay: 20
    until:
      - bmhs is succeeded
      - bmhs.resources | filter_provisioning("provisioning") | length == 1

  - name: Wait and verify that only 3 nodes are provisioned no other node are provisioning
    k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
    register: bmhs
    retries: 200
    delay: 20
    until:
      - bmhs is succeeded
      - bmhs.resources | filter_provisioning("provisioned") | length == 3
      - bmhs.resources | filter_provisioning("provisioning") | length == 0

  - name: Remove unhealthy annotation from "{{ WORKER_BMH }}"
    k8s:
      state: present
      namespace: "{{ NAMESPACE }}"
      definition:
        apiVersion: metal3.io/v1alpha1
        kind: BareMetalHost
        metadata:
          name: "{{ WORKER_BMH }}"
          annotations:
            capi.metal3.io/unhealthy: null

  - name: Verify that all machines are provisioned and running.
    include_tasks: verify_resources_states.yml
    vars:
      kubeconfig: "{{ HOME }}/.kube/config"

  - name: Scale down the machinedeployment
    k8s:
      api_version: cluster.x-k8s.io/{{ CAPI_VERSION }}
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      definition:
        spec:
          replicas: 1

  - name: Wait until two BMHs are in available state
    k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
    register: bmhs
    retries: 200
    delay: 20
    until:
      - bmhs is succeeded
      - bmhs.resources | filter_provisioning("available,ready") | length == 2

    ## Start Metal3DataTemplate reference test
  - name: Get the metal3datatemplate
    k8s_info:
      api_version: infrastructure.cluster.x-k8s.io/{{ CAPM3_VERSION }}
      kind: Metal3DataTemplate
      name: "{{ CLUSTER_NAME }}-workers-template"
      namespace: "{{ NAMESPACE }}"
    register: m3dt

  - name: Edit Metal3DataTemplate name and Add templateReference
    k8s:
      api_version: infrastructure.cluster.x-k8s.io/{{ CAPM3_VERSION }}
      kind: Metal3DataTemplate
      namespace: "{{ NAMESPACE }}"
      name: "{{ CLUSTER_NAME }}-workers-template"
      state: present
      definition: "{{ m3dt.resources[0] | edit_m3dt('test-new-m3dt', reference) }}"
    vars:
      - reference: "{{ CLUSTER_NAME }}-workers-template"

  - name: Get the Metal3MachineTemplate
    k8s_info:
      api_version: infrastructure.cluster.x-k8s.io/{{ CAPM3_VERSION }}
      kind: Metal3MachineTemplate
      name: "{{ CLUSTER_NAME }}-workers"
      namespace: "{{ NAMESPACE }}"
    register: m3mt

  - name: Edit Metal3MachineTemplate name and Refer to new Metal3DataTemplate
    k8s:
      api_version: infrastructure.cluster.x-k8s.io/{{ CAPM3_VERSION }}
      kind: Metal3MachineTemplate
      name: "{{ CLUSTER_NAME }}-workers"
      namespace: "{{ NAMESPACE }}"
      definition: "{{ m3mt.resources[0] | edit_m3mt('test-new-m3mt', 'test-new-m3dt') }}"

  - name: Edit MachineDeployment to point to the new m3mt
    k8s:
      api_version: cluster.x-k8s.io/{{ CAPI_VERSION }}
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      definition:
        spec:
          template:
            spec:
              infrastructureRef:
                name: "test-new-m3mt"
          strategy:
            rollingUpdate:
              maxUnavailable: 1

  - pause:
      minutes: 1

  - name: Wait until two BMHs are in available state
    k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
    register: bmhs
    retries: 200
    delay: 20
    until:
      - bmhs is succeeded
      - bmhs.resources | filter_provisioning("available,ready") | length == 2

  - name: Check if one Metal3Data refers to the old template
    k8s_info:
      api_version: infrastructure.cluster.x-k8s.io/{{ CAPM3_VERSION }}
      kind: Metal3Data
      namespace: "{{ NAMESPACE }}"
    register: m3data
    retries: 5
    delay: 5
    vars:
      query: "[? spec.templateReference=='{{ CLUSTER_NAME }}-workers-template'].metadata.name"
    until:
      - m3data is succeeded
      - m3data.resources | json_query(query) | length == 1

  - name: Scale KCP back to three replicas
    k8s:
      api_version: controlplane.cluster.x-k8s.io/{{ CAPI_VERSION }}
      kind: KubeadmControlPlane
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      definition:
        spec:
          replicas: 3

  - name: Verify that all machines are provisioned and running.
    include_tasks: verify_resources_states.yml
    vars:
      kubeconfig: "{{ HOME }}/.kube/config"
