---
  - set_fact:
      NUMBER_OF_BMH_KCP: "{{ NUM_OF_MASTER_REPLICAS|int }}"
      NUMBER_OF_BMH_MD: "{{ NUM_OF_WORKER_REPLICAS|int }}"
      NUMBER_OF_ALL_BMH: "{{ NUM_OF_WORKER_REPLICAS|int + NUM_OF_MASTER_REPLICAS|int }}"

  - name: Scale worker down to 0 to start testing KubeadmControlPlane node reuse test scenario.
    shell: |
        kubectl scale machinedeployment "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=0
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until worker is scaled down and "{{ NUMBER_OF_BMH_MD }}" BMH is Ready.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: ready_hosts
    until: ready_hosts.stdout == NUMBER_OF_BMH_MD

  - name: Get provisioned BMH names and UUIDs mapping before upgrade in KubeadmControlPlane node reuse test scenario.
    shell: |
        kubectl get bmh -A -o json | jq '.items[]| select (.status.provisioning.state == "provisioned")
        | "metal3/"+.metadata.name+"="+"metal3://"+.metadata.uid' | cut -f2 -d\" | sort > /tmp/before_upgrade_mapping.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update maxSurge field in KubeadmControlPlane to 0 to start testing KCP scale-in feature. 
    shell: |
        kubectl get kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.rolloutStrategy.rollingUpdate.maxSurge=0' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update Metal3MachineTemplate nodeReuse field to 'True'.
    shell: |
        kubectl get m3mt "{{ CLUSTER_NAME }}"-controlplane -n "{{ NAMESPACE }}" -ojson | jq '.spec.nodeReuse=true' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Upgrade KubeadmControlPlane k8s version from "{{ KUBERNETES_VERSION }}" to "{{ UPGRADED_K8S_VERSION }}" to test KCP scale-in feature.
    shell: |
        kubectl get kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.version="{{ UPGRADED_K8S_VERSION }}"' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - pause:
      minutes: 1

  - name: Check if any of the machines is in Provisioning state.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[] | select (.status.phase == "Provisioning") | .metadata.name ] | length'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: provisioning_machine

  - name: Check if only single machine is in Deleting state and no other new machine is in Provisioning state while upgrade triggered to test the KCP scale-in feature.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[] | select (.status.phase == "Deleting" or .status.phase == "deleting") | .metadata.name ] | length'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: deleting_machines
    retries: 200
    delay: 2
    until: deleting_machines.stdout == NUMBER_OF_BMH_MD
    failed_when: provisioning_machine.stdout != "0"

  - name: Wait until "{{ NUMBER_OF_BMH_MD }}" BMH is in deprovisioning state.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[]
        | select (.status.provisioning.state == "deprovisioning")
        | .metadata.name ] | length'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: bmh_in_deprovisioning
    retries: 150
    delay: 10
    until: bmh_in_deprovisioning.stdout == NUMBER_OF_BMH_MD

  - name: Get the name of the deprovisioning BMH.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[]
        | select (.status.provisioning.state == "deprovisioning")
        | .metadata.name ] | add'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: deprovisioning_bmh_name

  - name: Wait until above deprovisioning BMH is in Ready state again.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | awk '{print $1}'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: ready_bmh
    retries: 200
    delay: 2
    until: deprovisioning_bmh_name.stdout in ready_bmh.stdout_lines

  - name: Check if just deprovisioned and became ready BMH is re-used for next provisioning.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioning | awk '{print $1}'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: provisioning_bmh
    retries: 150
    delay: 2
    until: deprovisioning_bmh_name.stdout in provisioning_bmh.stdout_lines

  - name: Wait until two machines become running and updated with new "{{ UPGRADED_K8S_VERSION }}" k8s version.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | select(.spec.version == "{{ UPGRADED_K8S_VERSION }}") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: updated_machines_partly
    retries: 200
    delay: 20
    ignore_errors: yes
    until: updated_machines_partly.stdout|int > 1

  - pause:
      minutes: 5

  - name: Untaint all CP nodes after upgrade of two controlplane nodes.
    shell: |
        kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Wait until all "{{ NUMBER_OF_BMH_KCP }}" machines become running and updated with new "{{ UPGRADED_K8S_VERSION }}" k8s version.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | select(.spec.version == "{{ UPGRADED_K8S_VERSION }}") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: updated_machines_all
    retries: 200
    delay: 20
    ignore_errors: yes
    until: updated_machines_all.stdout|int == 3

  - name: Get provisioned BMH names and UUIDs mapping after upgrade in KubeadmControlPlane node reuse test scenario.
    shell: |
        kubectl get bmh -A -o json | jq '.items[]| select (.status.provisioning.state == "provisioned")
        | "metal3/"+.metadata.name+"="+"metal3://"+.metadata.uid' | cut -f2 -d\" | sort > /tmp/after_upgrade_mapping.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Check diff of before and after upgrade mappings to make sure same BMHs' were reused in KubeadmControlPlane node reuse test scenario.
    shell: |
        diff /tmp/before_upgrade_mapping.txt /tmp/after_upgrade_mapping.txt
    register: diff_mapping
    failed_when: diff_mapping.rc == 1

  - name: Finish KubeadmControlPlane node reuse test scenario and remove all related temp files.
    ansible.builtin.file:
      path: "{{ item }}"
      state: absent
    with_items:
      - /tmp/before_upgrade_mapping.txt
      - /tmp/after_upgrade_mapping.txt

  - name: Put maxSurge field in KubeadmControlPlane back to default value(1).
    shell: |
        kubectl get kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.rolloutStrategy.rollingUpdate.maxSurge=1' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Scale controlplane down to 1.
    shell: |
        kubectl scale kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=1
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - pause:
      minutes: 5

  - name: Untaint all CP nodes.
    shell: |
        kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Wait until controlplane is scaled down and "{{ NUMBER_OF_BMH_KCP }}" BMHs' are Ready.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: all_ready_hosts
    until: all_ready_hosts.stdout == "3"

  - name: Scale worker up to "{{ NUMBER_OF_BMH_MD }}" to start testing MachineDeployment node reuse test scenario.
    shell: |
        kubectl scale machinedeployment "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=1
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until "{{ NUMBER_OF_BMH_MD }}" more BMH is provisioned.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioned | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: provisioned_host
    until: provisioned_host.stdout == "2"

  - name: Wait until "{{ NUMBER_OF_BMH_MD }}" more machine becomes running.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: provisioned_machine
    retries: 200
    delay: 20
    until: provisioned_machine.stdout|int == 2

  - name: Get provisioned BMH names and UUIDs mapping before upgrade in MachineDeployment node reuse test scenario.
    shell: |
        kubectl get bmh -A -o json | jq '.items[]| select (.status.provisioning.state == "provisioned")
        | "metal3/"+.metadata.name+"="+"metal3://"+.metadata.uid' | cut -f2 -d\" | sort > /tmp/before_upgrade_mapping_md.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update maxSurge and maxUnavailable fields in MachineDeployment.
    shell: |
        kubectl get machinedeployment "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.strategy.rollingUpdate.maxSurge=0|.spec.strategy.rollingUpdate.maxUnavailable=1' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update Metal3MachineTemplate nodeReuse field to 'True'.
    shell: |
        kubectl get m3mt "{{ CLUSTER_NAME }}"-workers -n "{{ NAMESPACE }}" -ojson | jq '.spec.nodeReuse=true' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: List only ready BMHs'.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[] | select (.status.provisioning.state == "ready") | .metadata.name ]'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: ready_bmhs

  - name: Mark all ready BMHs' with unhealthy annotation.
    shell: |
        kubectl annotate bmh "{{ item }}" -n "{{ NAMESPACE }}" capi.metal3.io/unhealthy=
    loop: "{{ ready_bmhs.stdout }}"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Upgrade MachineDeployment k8s version from "{{ KUBERNETES_VERSION }}" to "{{ UPGRADED_K8S_VERSION }}".
    shell: |
        kubectl get machinedeployment "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.template.spec.version="{{ UPGRADED_K8S_VERSION }}"' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until "{{ NUMBER_OF_BMH_MD }}" BMH is in deprovisioning state.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[] | select (.status.provisioning.state == "deprovisioning") | .metadata.name ] | length'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: bmh_in_deprovisioning
    retries: 150
    delay: 10
    until: bmh_in_deprovisioning.stdout == NUMBER_OF_BMH_MD

  - name: Get the name of the deprovisioning BMH.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[]
        | select (.status.provisioning.state == "deprovisioning")
        | .metadata.name ] | add'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: deprovisioning_bmh_name

  - name: Wait until above deprovisioning BMH is in Ready state again.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | awk '{print $1}'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: ready_bmh
    retries: 200
    delay: 2
    until: deprovisioning_bmh_name.stdout in ready_bmh.stdout_lines

  - name: Unmark all ready BMHs' with unhealthy annotation.
    shell: |
        kubectl annotate bmh "{{ item }}" -n "{{ NAMESPACE }}" capi.metal3.io/unhealthy-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    loop: "{{ ready_bmhs.stdout }}"

  - name: Check if just deprovisioned and became ready BMH is re-used for next provisioning.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioning | awk '{print $1}'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: provisioning_bmh
    retries: 200
    delay: 2
    until: deprovisioning_bmh_name.stdout in provisioning_bmh.stdout_lines

  - name: Wait until worker machine becomes running and updated with new "{{ UPGRADED_K8S_VERSION }}" k8s version.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | select(.spec.version == "{{ UPGRADED_K8S_VERSION }}") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: updated_machines
    retries: 200
    delay: 20
    until: updated_machines.stdout|int == 2

  - name: Get provisioned BMH names and UUIDs mapping after upgrade in MachineDeployment node reuse test scenario.
    shell: |
        kubectl get bmh -A -o json | jq '.items[]| select (.status.provisioning.state == "provisioned")
        | "metal3/"+.metadata.name+"="+"metal3://"+.metadata.uid' | cut -f2 -d\" | sort > /tmp/after_upgrade_mapping_md.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Check diff of before and after upgrade mapping to make sure same BMH reused in MachineDeployment node reuse test scenario.
    shell: |
        diff /tmp/before_upgrade_mapping_md.txt /tmp/after_upgrade_mapping_md.txt
    register: diff_mapping
    failed_when: diff_mapping.rc == 1

  - name: Finish MachineDeployment node reuse test scenario and remove all related temp files.
    ansible.builtin.file:
      path: "{{ item }}"
      state: absent
    with_items:
      - /tmp/before_upgrade_mapping_md.txt
      - /tmp/after_upgrade_mapping_md.txt

  - name: Scale controlplane up back to 3.
    shell: |
        kubectl scale kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=3
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until all "{{ NUMBER_OF_ALL_BMH }}" BMHs' are provisioned.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioned | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: provisioned_host
    until: provisioned_host.stdout == "4"

  - name: Wait until all "{{ NUMBER_OF_ALL_BMH }}" machines become running.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: running_machines
    retries: 200
    delay: 20
    until: running_machines.stdout|int == 4