---
  - name: Define number of BMH's
    set_fact:
      NUMBER_OF_BMH: "{{ NUM_OF_MASTER_REPLICAS|int +  NUM_OF_WORKER_REPLICAS|int }}"

  - name: Update maxSurge and maxUnavailable fields
    kubernetes.core.k8s:
      api_version: cluster.x-k8s.io/v1alpha3
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      resource_definition:
        spec:
          strategy:
            rollingUpdate:
              maxSurge: 1
              maxUnavailable: 1
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Untaint all nodes
    ansible.builtin.command: kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Scale worker down to 0
    kubernetes.core.k8s:
      api_version: cluster.x-k8s.io/v1alpha3
      kind: MachineDeployment
      name: "{{ CLUSTER_NAME }}"
      namespace: "{{ NAMESPACE }}"
      resource_definition:
        spec:
          replicas: 0
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until worker is scaled down and one bmh is Ready
    kubernetes.core.k8s_info:
      api_version: metal3.io/v1alpha1
      kind: BareMetalHost
      namespace: "{{ NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: bare_metal_hosts
    vars:
      query: "[? status.provisioning.state=='ready']"
    until: (bare_metal_hosts is succeeded) and
           (bare_metal_hosts.resources | json_query(query) | length == 1)

# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
#                       Upgrade controlplane components                                 |
# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
  - name: Gather variables that were missing on the source cluster
    kubernetes.core.k8s_info:
      api_version: v1
      kind: ConfigMap
      namespace: "{{ NAMEPREFIX }}-system"
      name: "{{ NAMEPREFIX }}-baremetal-operator-ironic"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: configmap

  - name: Store ironic env for later use
    # We need to replace "ENDPOINT" with "URL" for some of the keys in the dict.
    # This is done by splitting the dict up into separate keys and values lists
    # that can then be manipulated with "map".
    set_fact:
      ironic_env:
        # Reconstruct the dict with the modified keys
        "{{ dict(keys | zip(values)) }}"
    vars:
      # Replace "ENDPOINT" with "URL" for the keys.
      keys: "{{ configmap.resources[0].data.keys() | map('regex_replace', 'ENDPOINT', 'URL') | list }}"
      values: "{{ configmap.resources[0].data.values() | list }}"


  - name: Backup ironic credentials for re-use when pods are re-created during the upgrade process
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Secret
      namespace: "{{ NAMEPREFIX }}-system"
      label_selectors:
        - cluster.x-k8s.io/provider = infrastructure-metal3
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: ironic_credentials

  - name: Cleanup - remove existing next versions of controlplane components CRDs
    vars:
      working_dir: "{{HOME}}/.cluster-api/dev-repository/"
    file:
      state: absent
      path: "{{item}}"
    with_items:
    - "{{working_dir}}/cluster-api/{{CAPI_REL_TO_VERSION}}/"
    - "{{working_dir}}/bootstrap-kubeadm/{{CAPI_REL_TO_VERSION}}"
    - "{{working_dir}}/control-plane-kubeadm/{{CAPI_REL_TO_VERSION}}"
    - "{{working_dir}}/infrastructure-metal3/{{CAPM3_REL_TO_VERSION}}"

  - name: Generate clusterctl configuration file
    ansible.builtin.template:
      src: clusterctl-upgrade-test.yaml
      dest: "{{HOME}}/.cluster-api/clusterctl.yaml"

  - name: Get clusterctl repo
    ansible.builtin.git:
      repo: 'https://github.com/kubernetes-sigs/cluster-api.git'
      dest: /tmp/cluster-api-clone
      version: "{{ CAPIRELEASE }}"

  - name: Create clusterctl-settings.json for cluster-api and capm3 repo
    ansible.builtin.template:
      src: "{{ item.src }}"
      dest: "{{ item.dest }}"
    loop:
      - src: cluster-api-clusterctl-settings.json
        dest: /tmp/cluster-api-clone/clusterctl-settings.json
      - src: capm3-clusterctl-settings.json
        dest: "{{ M3PATH }}/clusterctl-settings.json"

  - name: Build clusterctl binary
    ansible.builtin.command: make clusterctl
    args:
      chdir: /tmp/cluster-api-clone/

  - name: Copy clusterctl to /usr/local/bin
    ansible.builtin.copy:
      src: /tmp/cluster-api-clone/bin/clusterctl
      dest: /usr/local/bin/clusterctl
      remote_src: yes
      owner: root
      mode: u=rwx,g=rx,o=rx
    become: yes
    become_user: root

  - name: Create local repository
    ansible.builtin.command: cmd/clusterctl/hack/create-local-repository.py
    args:
      chdir: /tmp/cluster-api-clone/
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Create folder structure for next version controlplane components
    vars:
      working_dir: "{{HOME}}/.cluster-api"
    file:
      path: "{{ working_dir }}/{{ item }}"
      state: directory
      recurse: yes
    with_items:
    - dev-repository/cluster-api/{{ CAPI_REL_TO_VERSION }}
    - dev-repository/bootstrap-kubeadm/{{ CAPI_REL_TO_VERSION }}
    - dev-repository/control-plane-kubeadm/{{ CAPI_REL_TO_VERSION }}
    - overrides/infrastructure-metal3/{{ CAPM3_REL_TO_VERSION }}

  - name: Create next version controller CRDs
    vars:
      working_dir: "{{HOME}}/.cluster-api/"
    copy: src="{{working_dir}}/{{item.src}}" dest="{{working_dir}}/{{item.dest}}"
    with_items:
    - {
        src: "dev-repository/cluster-api/{{CAPIRELEASE_HARDCODED}}/core-components.yaml",
        dest: "dev-repository/cluster-api/{{CAPI_REL_TO_VERSION}}/core-components.yaml"
      }
    - {
        src: "dev-repository/bootstrap-kubeadm/{{CAPIRELEASE_HARDCODED}}/bootstrap-components.yaml",
        dest: "dev-repository/bootstrap-kubeadm/{{CAPI_REL_TO_VERSION}}/bootstrap-components.yaml"
      }
    - {
        src: "dev-repository/control-plane-kubeadm/{{CAPIRELEASE_HARDCODED}}/control-plane-components.yaml",
        dest: "dev-repository/control-plane-kubeadm/{{CAPI_REL_TO_VERSION}}/control-plane-components.yaml"
      }
    - {
        src: "overrides/infrastructure-metal3/{{ CAPM3RELEASE }}/infrastructure-components.yaml",
        dest: "overrides/infrastructure-metal3/{{ CAPM3_REL_TO_VERSION }}/infrastructure-components.yaml"
      }

  - name: Make changes on CRDs
    vars:
      working_dir: "{{HOME}}/.cluster-api"
    ansible.builtin.replace:
      path: "{{ item.path }}"
      regexp: "{{ item.regexp }}"
      replace: "{{ item.replace }}"
    loop:
      - path: "{{working_dir}}/dev-repository/cluster-api/{{CAPI_REL_TO_VERSION}}/core-components.yaml"
        regexp: 'description: Machine'
        replace: "description: upgradedMachine"
      - path: "{{working_dir}}/dev-repository//bootstrap-kubeadm/{{CAPI_REL_TO_VERSION}}/bootstrap-components.yaml"
        regexp: 'description: KubeadmConfig'
        replace: "description: upgradedKubeadmConfig"
      - path: "{{working_dir}}/dev-repository//control-plane-kubeadm/{{CAPI_REL_TO_VERSION}}/control-plane-components.yaml"
        regexp: 'description: KubeadmControlPlane'
        replace: "description: upgradedKubeadmControlPlane"
      # TODO: Should we use something more generic than "m3c2020"? Say "m3cnext" or "upgm3c"?
      - path: "{{working_dir}}/dev-repository/infrastructure-metal3/{{CAPM3_REL_TO_VERSION}}/infrastructure-components.yaml"
        regexp: '\bm3c\b'
        replace: "m3c2020"
      - path: "{{working_dir}}/overrides/infrastructure-metal3/{{CAPM3_REL_TO_VERSION}}/infrastructure-components.yaml"
        regexp: '\bm3c\b'
        replace: "m3c2020"

  - name: Perform upgrade on the target cluster
    ansible.builtin.command: clusterctl upgrade apply --management-group capi-system/cluster-api --contract v1alpha3
    environment: "{{ ironic_env | combine(kubeconfig_env) }}"
    vars:
      kubeconfig_env:
        KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  # TODO: Can we check this somehow instead of just waiting?
  # Relevant issue: https://github.com/kubernetes-sigs/cluster-api/issues/4474
  - name: Wait for upgrade on the target cluster
    ansible.builtin.pause:
      seconds: 30

  - name: Restore secrets after upgrade of the target cluster
    kubernetes.core.k8s:
      state: present
      force: yes
      resource_definition: "{{ ironic_credentials.resources | k8s_backup }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Perform upgrade on the source cluster
    ansible.builtin.command: clusterctl upgrade apply --management-group capi-system/cluster-api --contract v1alpha3
    environment: "{{ ironic_env }}"

  # TODO: Can we check this somehow instead of just waiting?
  # Relevant issue: https://github.com/kubernetes-sigs/cluster-api/issues/4474
  - name: Wait for upgrade on the source cluster
    ansible.builtin.pause:
      seconds: 30

  - name: Restore secrets after upgrade of the target cluster
    kubernetes.core.k8s:
      state: present
      force: yes
      resource_definition: "{{ ironic_credentials.resources | k8s_backup }}"

  - name: Verify that CP components are updated and available
    k8s_info:
      api_version: v1
      kind: Deployment
      label_selectors:
        - clusterctl.cluster.x-k8s.io
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: controller_deployments
    retries: 200
    delay: 20
    vars:
      # Select any item where updatedReplicas or availableReplicas is different from replicas.
      # This would indicate that there are some unhealthy pods or a stuck/failed rollout.
      query: "[?(status.updatedReplicas != status.replicas) || (status.availableReplicas != status.replicas)]"
    until: (controller_deployments is succeeded) and
           (controller_deployments.resources | json_query(query) | length == 0)

  - name: Verify that CRDs are upgraded
    kubernetes.core.k8s_info:
      api_version: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: "{{ item.name }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: crd
    vars:
      query: "[?name == 'v1alpha3'].schema.openAPIV3Schema.description"
    failed_when: crd.resources[0].spec.versions | json_query(query) is not search("{{ item.search }}")
    loop:
      - name: machines.cluster.x-k8s.io
        search: upgradedMachine
      - name: kubeadmcontrolplanes.controlplane.cluster.x-k8s.io
        search: upgradedKubeadmControlPlane
      - name: kubeadmconfigs.bootstrap.cluster.x-k8s.io
        search: upgradedKubeadmConfig

  # Due to https://github.com/ansible-collections/kubernetes.core/issues/17
  # we cannot use k8s_cluster_info. A fix is to be included in 2.0.0 of kubernetes.core
  # - name: Verify upgraded API resource for Metal3Clusters
  #   kubernetes.core.k8s_cluster_info:
  #   register: api_status
  #   # failed_when: api_status.apis not contains the upgraded m3c resource(s)?

  - name: Verify upgraded API resource for Metal3Clusters
    # spec.names.shortNames[]
    # status.acceptedNames.shortNames[]
    kubernetes.core.k8s_info:
      api_version: apiextensions.k8s.io/v1
      kind: CustomResourceDefinition
      name: metal3clusters.infrastructure.cluster.x-k8s.io
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: crd
    failed_when: '"m3c2020" not in crd.resources[0].spec.names.shortNames'

# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
#                       Upgrade Ironic                                                  |
# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
  - name: Set expected ironic image based containers
    set_fact:
      ironic_image_containers:
        - mariadb
        - ironic-api
        - ironic-dnsmasq
        - ironic-conductor
        - ironic-log-watch
        - ironic-inspector
        - ironic-inspector-log-watch
        # There is also a keepalived container in the pods, but it is using a
        # different image than the rest and therefore not included in the list.
        # - ironic-endpoint-keepalived

  - name: Upgrade ironic image based containers
    kubernetes.core.k8s:
      api_version: v1
      kind: Deployment
      name: "{{ NAMEPREFIX }}-ironic"
      namespace: "{{ IRONIC_NAMESPACE }}"
      resource_definition:
        spec:
          template:
            spec:
              containers: "{{ containers }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    vars:
      # Generate a list of name/image pairs from the ironic_image_containers.
      # This is to avoid looping which would create one new revision for each container.
      # 1. Zip to get a list of lists: [[container_name, image_name], [...]]
      #    (all have the same image)
      # 2. Turn it into a dict so we have {container_name: image_name, ...}
      # 3. Convert it to a list of {name: container_name, image: image_name}
      containers:
        "{{ dict(ironic_image_containers |
              zip_longest([], fillvalue='quay.io/metal3-io/ironic:'+IRONIC_IMAGE_TAG)) |
            dict2items(key_name='name', value_name='image') }}"

  - name: Wait for ironic update to rollout
    kubernetes.core.k8s_info:
      api_version: v1
      kind: Deployment
      name: "{{ NAMEPREFIX }}-ironic"
      namespace: "{{ IRONIC_NAMESPACE }}"
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 100
    delay: 10
    register: ironic_deployment
    # We are checking that there is 1 updated replica, that it is available, and
    # that it is the only one (so no old replica left).
    # Note that the these fields can be missing if the controller didn't have
    # time to update them yet, so we need to set a default value.
    until: (ironic_deployment is succeeded) and
           (ironic_deployment.resources | length > 0) and
           (ironic_deployment.resources[0].status.updatedReplicas | default(0) == 1) and
           (ironic_deployment.resources[0].status.availableReplicas | default(0) == 1) and
           (ironic_deployment.resources[0].status.replicas | default(0) == 1)

# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
#                       Upgrade K8S version and boot-image                              |
# ---------- ---------- ---------- ---------- ---------- ---------- ---------- ----------
  - name: get cluster uid
    shell: |
            kubectl get clusters {{ CLUSTER_NAME }}  -n {{NAMESPACE}} -o json | jq '.metadata.uid' |   cut -f2 -d\"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: CLSTR_UID

  - name: Genenrate controlplane Metal3MachineTemplate
    vars:
       CLUSTER_UID: "{{ CLSTR_UID.stdout }}"
       M3MT_NAME: "{{CLUSTER_NAME}}-new-controlplane-image"
       DATA_TEMPLATE_NAME: "{{CLUSTER_NAME}}-controlplane-template"
    template:
      src: Metal3MachineTemplate.yml
      dest: /tmp/cp_new_image.yaml

  - name: Genenrate worker Metal3MachineTemplate
    vars:
      CLUSTER_UID: "{{ CLSTR_UID.stdout_lines[0] }}"
      M3MT_NAME: "{{CLUSTER_NAME}}-new-workers-image"
      DATA_TEMPLATE_NAME: "{{CLUSTER_NAME}}-workers-template"
    template:
      src: Metal3MachineTemplate.yml
      dest: /tmp/wr_new_image.yaml

  - name: Create controlplane and worker Metal3MachineTemplates
    kubernetes.core.k8s:
      state: present
      src: /tmp/cp_new_image.yaml
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Create controlplane and worker Metal3MachineTemplates
    kubernetes.core.k8s:
      state: present
      src:  /tmp/wr_new_image.yaml
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update boot-disk and kubernetes versions of controlplane nodes
    shell: |
            kubectl get kubeadmcontrolplane -n {{NAMESPACE}} {{ CLUSTER_NAME }} -o json |
            jq '.spec.infrastructureTemplate.name="{{CLUSTER_NAME}}-new-controlplane-image" |
               .spec.version="{{UPGRADED_K8S_VERSION}}"'|
                kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Verify that controlplane nodes using the new node image
    shell: |
            kubectl get bmh -n {{NAMESPACE}} |
            grep -i provisioned | grep -c 'new-controlplane-image'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: new_image_cp_nodes
    until: new_image_cp_nodes.stdout|int == 3
    failed_when: new_image_cp_nodes.stdout|int != 3

  - name: Untaint all CP nodes after upgrade of controlplane nodes
    command: kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Wait for old etcd instance to leave the new etcd-cluster
    pause:
      minutes: 10

  - name: Verify that the old controlplane node has left the cluster
    shell: |
            kubectl get bmh -n {{NAMESPACE}} | grep -i provisioned | grep -c "{{ CLUSTER_NAME }}-controlplane-"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: upgraded_cp_nodes_count
    until: upgraded_cp_nodes_count.stdout|int == 0
    failed_when: upgraded_cp_nodes_count.stdout|int != 0

  - name: Wait for old etcd instance to leave the new etcd-cluster
    pause:
      minutes: 10

  - name: Scale worker up to 1
    shell: |
        kubectl scale machinedeployment "{{ CLUSTER_NAME }}"  -n "{{ NAMESPACE }}" --replicas=1
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until worker is scaled up and no bmh is in Ready state
    shell: kubectl get node | awk 'NR>1'| grep -cv master
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: worker_nodes
    until: worker_nodes.stdout|int == 1
    failed_when: worker_nodes.stdout|int == 0

  - name: Label worker for scheduling purpose
    shell: |
      WORKER_NAME=$(kubectl get nodes -n {{NAMESPACE}} | awk 'NR>1'| grep -v master | awk '{print $1}')
      kubectl label node "${WORKER_NAME}" type=worker
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Copy workload manifest to /tmp
    copy:
      src: workload.yaml
      dest: /tmp/workload.yaml

  - name: Deploy workload with nodeAffinity
    kubernetes.core.k8s:
      state: present
      src: /tmp/workload.yaml
      namespace: default
      kubeconfig: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - pause:
      minutes: 5

  - name: Show workload deployment status on worker node
    shell: |
            kubectl get pods | grep 'workload-1-deployment'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Verify workload deployment
    shell: |
            kubectl get deployments workload-1-deployment -o json | jq '.status.readyReplicas'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: running_workload_pods
    until: running_workload_pods.stdout|int == 10
    failed_when: running_workload_pods.stdout|int != 10

  - name: Update boot-disk and kubernetes versions of worker node
    shell: |
            kubectl get machinedeployment -n {{NAMESPACE}} {{ CLUSTER_NAME }} -o json |
            jq '.spec.template.spec.infrastructureRef.name="{{ CLUSTER_NAME }}-new-workers-image" |
            .spec.template.spec.version="{{UPGRADED_K8S_VERSION}}"'| kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Verify that worker node is using the new boot-image
    shell: |
            kubectl get bmh -n {{NAMESPACE}} |
            grep -i provisioned | grep -c 'new-workers-image'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: new_image_wr_nodes
    until: new_image_wr_nodes.stdout|int == 1
    failed_when: new_image_wr_nodes.stdout|int != 1

  - name: Verify that the upgraded worker node has joined the cluster
    shell: |
            kubectl get nodes | awk 'NR>1'| grep -vc master
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: joined_wr_node
    until: joined_wr_node.stdout|int == 1
    failed_when: joined_wr_node.stdout|int != 1

  - name: Verify that kubernetes version is upgraded for CP and worker nodes
    shell: |
            kubectl get machines -n {{NAMESPACE}} -o json |
            jq '.items[].spec.version' | cut -f2 -d\" | sort -u
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: upgrade_k8s_version
    failed_when: upgrade_k8s_version.stdout != "{{ UPGRADED_K8S_VERSION }}"
